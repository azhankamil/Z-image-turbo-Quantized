# Z-Image-Turbo on GTX 1050 Ti (4GB) â€“ ComfyUI Workflow

This repo shows something simple:

> A 2016 GPU can still generate AI images in 2026.


<img width="1440" height="779" alt="Screenshot 2026-02-27 at 6 45 13â€¯AM" src="https://github.com/user-attachments/assets/697886af-881c-4a2d-8e8d-6b8eb38bd8b7" />

No RTX.  
No 12GB VRAM.  
No fancy AI marketing.  

**Just optimization.**

---

## ğŸ–¥ï¸ Hardware Used

- **GPU:** GTX 1050 Ti (4GB, Pascal â€“ 2016)
- **CPU:** i5-9400F
- **RAM:** 16GB
- **UI:** ComfyUI
- **Model:** Z-Image-Turbo (FP8)

---

## ğŸš€ Whatâ€™s Happening Here?

Modern diffusion models expect big GPUs.

This setup doesnâ€™t have one.

Instead, it works because of:

- **FP8 quantization** â†’ smaller model memory footprint  
- **VRAM â†” RAM offloading** â†’ avoids CUDA OOM crashes  
- **512Ã—512 sweet spot** â†’ manageable activation memory  
- **Lean workflow** â†’ no unnecessary nodes  

### Result

- ~2.4GB loaded on GPU  
- Remaining weights offloaded to RAM  
- Stable image generation on 4GB VRAM  

---

## ğŸ¯ Recommended Settings

| Setting        | Value                         |
|---------------|--------------------------------|
| Resolution     | 512Ã—512                        |
| Steps          | 6â€“8                            |
| Precision      | FP8                            |
| Launch Flags   | `--lowvram --fp8_e4m3fn`        |
| Offloading     | Enabled                        |

> If it crashes:
> - Lower resolution first  
> - Then reduce steps  

---

## ğŸ“¦ Required Models

### 1ï¸âƒ£ Z-Image-Turbo (FP8)

Place inside:
Queue prompt.
Thatâ€™s it.
